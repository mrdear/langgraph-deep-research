import os
import json
from typing import List
from datetime import datetime

from agent.tools_and_schemas import SearchQueryList, Reflection, ResearchPlan, LedgerEntry
from dotenv import load_dotenv
from langchain_core.messages import AIMessage
from langgraph.types import Send
from langgraph.graph import StateGraph
from langgraph.graph import START, END
from langchain_core.runnables import RunnableConfig
from google.genai import Client
import tiktoken  # éœ€ç¡®ä¿ç¯å¢ƒå·²å®‰è£… tiktoken

from agent.state import (
    OverallState,
    QueryGenerationState,
    ReflectionState,
    WebSearchState,
)
from agent.configuration import Configuration
from agent.prompts import (
    get_current_date,
    query_writer_instructions,
    web_searcher_instructions,
    reflection_instructions,
    answer_instructions,
    planning_instructions,
)
from langchain_google_genai import ChatGoogleGenerativeAI
from agent.utils import (
    get_citations,
    get_research_topic,
    insert_citation_markers,
    resolve_urls,
)
# å¯¼å…¥æ™ºèƒ½å†…å®¹å¢å¼ºæ¨¡å—
from agent.enhanced_graph_nodes import (
    content_enhancement_analysis,
    should_enhance_content
)

load_dotenv()

if os.getenv("GEMINI_API_KEY") is None:
    raise ValueError("GEMINI_API_KEY is not set")

# Used for Google Search API
genai_client = Client(api_key=os.getenv("GEMINI_API_KEY"))


# Nodes
def generate_query(state: OverallState, config: RunnableConfig) -> QueryGenerationState:
    """LangGraph node that generates search queries based on the current research task from the plan."""
    configurable = Configuration.from_runnable_config(config)

    # check for custom initial search query count
    if state.get("initial_search_query_count") is None:
        state["initial_search_query_count"] = configurable.number_of_initial_queries

    # init Gemini 2.0 Flash
    llm = ChatGoogleGenerativeAI(
        model=configurable.query_generator_model,
        temperature=1.0,
        max_retries=2,
        api_key=os.getenv("GEMINI_API_KEY"),
    )
    structured_llm = llm.with_structured_output(SearchQueryList)

    # New logic: prioritize generating queries based on current plan task
    plan = state.get("plan")
    pointer = state.get("current_task_pointer")
    if plan and pointer is not None and pointer < len(plan):
        research_topic = plan[pointer]["description"]
    else:
        # Fallback to user_query or messages
        research_topic = state.get("user_query") or get_research_topic(state["messages"])

    current_date = get_current_date()
    formatted_prompt = query_writer_instructions.format(
        current_date=current_date,
        research_topic=research_topic,
        number_queries=state["initial_search_query_count"],
    )
    result = structured_llm.invoke(formatted_prompt)
    
    return {
        "query_list": result.query,
        "plan": state.get("plan", []),
        "current_task_pointer": state.get("current_task_pointer", 0)
    }


def continue_to_web_research(state: QueryGenerationState):
    """LangGraph node that sends the search queries to the web research node.

    This is used to spawn n number of web research nodes, one for each search query.
    """
    # Get current task info
    plan = state.get("plan", [])
    current_pointer = state.get("current_task_pointer", 0)
    current_task_id = "unknown"
    
    if plan and current_pointer < len(plan):
        current_task_id = plan[current_pointer]["id"]
    
    return [
        Send("web_research", {
            "search_query": search_query, 
            "id": int(idx),
            "current_task_id": current_task_id
        })
        for idx, search_query in enumerate(state["query_list"])
    ]


def web_research(state: WebSearchState, config: RunnableConfig) -> OverallState:
    """LangGraph node that performs web research using the native Google Search API tool.

    Executes a web search using the native Google Search API tool in combination with Gemini 2.0 Flash.

    Args:
        state: Current graph state containing the search query and research loop count
        config: Configuration for the runnable, including search API settings

    Returns:
        Dictionary with state update, including sources_gathered, research_loop_count, and web_research_results
    """
    try:
        # Configure
        configurable = Configuration.from_runnable_config(config)
        formatted_prompt = web_searcher_instructions.format(
            current_date=get_current_date(),
            research_topic=state["search_query"],
        )

        # Uses the google genai client as the langchain client doesn't return grounding metadata
        response = genai_client.models.generate_content(
            model=configurable.query_generator_model,
            contents=formatted_prompt,
            config={
                "tools": [{"google_search": {}}],
                "temperature": 0,
            },
        )
        
        # Error handling for empty response
        if not response.candidates or not response.candidates[0].grounding_metadata:
            current_task_id = state.get("current_task_id", "unknown")
            error_content = f"No results found for query: {state['search_query']}"
            
            detailed_finding = {
                "task_id": current_task_id,
                "query_id": state["id"],
                "content": error_content,
                "source": None,
                "timestamp": datetime.now().isoformat()
            }
            
            task_specific_result = {
                "task_id": current_task_id,
                "content": error_content,
                "sources": [],
                "timestamp": datetime.now().isoformat()
            }
            
            return {
                "sources_gathered": [],
                "executed_search_queries": [state["search_query"]],
                "web_research_result": [error_content],
                "current_task_detailed_findings": [detailed_finding],
                "task_specific_results": [task_specific_result]
            }

        # resolve the urls to short urls for saving tokens and time
        resolved_urls = resolve_urls(
            response.candidates[0].grounding_metadata.grounding_chunks, state["id"]
        )
        
        # Gets the citations and adds them to the generated text
        citations = get_citations(response, resolved_urls)
        modified_text = insert_citation_markers(response.text, citations)
        sources_gathered = [item for citation in citations for item in citation["segments"]]

        # Create detailed findings entry with task ID
        current_task_id = state.get("current_task_id", "unknown")
        detailed_finding = {
            "task_id": current_task_id,
            "query_id": state["id"],
            "content": modified_text,
            "source": sources_gathered[0] if sources_gathered else None,
            "timestamp": datetime.now().isoformat()
        }

        # Add task-specific metadata to the research result
        task_specific_result = {
            "task_id": current_task_id,
            "content": modified_text,
            "sources": sources_gathered,
            "timestamp": datetime.now().isoformat()
        }

        return {
            "sources_gathered": sources_gathered,
            "executed_search_queries": [state["search_query"]],
            "web_research_result": [modified_text],
            "current_task_detailed_findings": [detailed_finding],
            "task_specific_results": [task_specific_result]
        }
    except Exception as e:
        # Error handling for API or processing errors
        current_task_id = state.get("current_task_id", "unknown")
        error_message = f"Error during web research: {str(e)}"
        
        detailed_finding = {
            "task_id": current_task_id,
            "query_id": state["id"],
            "content": error_message,
            "source": None,
            "timestamp": datetime.now().isoformat()
        }
        
        task_specific_result = {
            "task_id": current_task_id,
            "content": error_message,
            "sources": [],
            "timestamp": datetime.now().isoformat()
        }
        
        return {
            "sources_gathered": [],
            "executed_search_queries": [state["search_query"]],
            "web_research_result": [error_message],
            "current_task_detailed_findings": [detailed_finding],
            "task_specific_results": [task_specific_result]
        }


def reflection(state: OverallState, config: RunnableConfig) -> OverallState:
    """LangGraph node that identifies knowledge gaps and generates potential follow-up queries.

    This is where we check if our search results are sufficient to answer the research question.
    If not, we generate follow-up queries to address the knowledge gap.
    """
    try:
        configurable = Configuration.from_runnable_config(config)
        
        # é€’å¢ç ”ç©¶å¾ªç¯è®¡æ•°
        state["research_loop_count"] = state.get("research_loop_count", 0) + 1
        
        reasoning_model = configurable.reasoning_model
        current_date = get_current_date()
        research_topic = get_research_topic(state["messages"])
        
        # å®‰å…¨åœ°è·å–web researchç»“æœï¼Œå¹¶æˆªæ–­è¿‡é•¿å†…å®¹
        web_research_results = state.get("web_research_result", [])
        
        # å†…å®¹æˆªæ–­ï¼šé™åˆ¶æ€»å­—ç¬¦æ•°ä»¥é¿å…APIé™åˆ¶
        MAX_CHARS = 50000  # çº¦12500 tokens
        truncated_results = []
        total_chars = 0
        
        for result in web_research_results:
            result_str = str(result)
            if total_chars + len(result_str) <= MAX_CHARS:
                truncated_results.append(result_str)
                total_chars += len(result_str)
            else:
                # éƒ¨åˆ†æˆªå–æœ€åä¸€ä¸ªç»“æœ
                remaining_chars = MAX_CHARS - total_chars
                if remaining_chars > 500:  # è‡³å°‘ä¿ç•™500å­—ç¬¦
                    truncated_results.append(result_str[:remaining_chars] + "...[truncated]")
                break
        
        print(f"ğŸ” Reflectionåˆ†æ: {len(web_research_results)} ä¸ªç»“æœï¼Œæˆªæ–­å {len(truncated_results)} ä¸ªï¼Œ{total_chars} å­—ç¬¦")
        
        formatted_prompt = reflection_instructions.format(
            current_date=current_date,
            research_topic=research_topic,
            summaries="\n\n---\n\n".join(truncated_results),
        )
        
        # æ£€æŸ¥prompté•¿åº¦
        prompt_length = len(formatted_prompt)
        print(f"ğŸ“ Reflection prompté•¿åº¦: {prompt_length} å­—ç¬¦")
        
        if prompt_length > 100000:  # å¦‚æœä»ç„¶è¿‡é•¿ï¼Œè¿›ä¸€æ­¥æˆªæ–­
            print("âš ï¸ Promptè¿‡é•¿ï¼Œè¿›ä¸€æ­¥æˆªæ–­summarieséƒ¨åˆ†")
            truncated_summaries = "\n\n---\n\n".join(truncated_results[:3])  # åªä¿ç•™å‰3ä¸ªç»“æœ
            formatted_prompt = reflection_instructions.format(
                current_date=current_date,
                research_topic=research_topic,
                summaries=truncated_summaries,
            )
        
        # åˆå§‹åŒ–LLM
        llm = ChatGoogleGenerativeAI(
            model=reasoning_model,
            temperature=1.0,
            max_retries=3,  # å¢åŠ é‡è¯•æ¬¡æ•°
            api_key=os.getenv("GEMINI_API_KEY"),
        )
        
        # å°è¯•ç»“æ„åŒ–è¾“å‡º
        try:
            print("ğŸ¤– æ­£åœ¨è°ƒç”¨Gemini APIè¿›è¡Œreflectionåˆ†æ...")
            result = llm.with_structured_output(Reflection).invoke(formatted_prompt)
            print("âœ… Reflectionåˆ†ææˆåŠŸå®Œæˆ")
            
        except Exception as api_error:
            print(f"âŒ Structured outputå¤±è´¥: {str(api_error)}")
            print("ğŸ”„ å°è¯•fallbackæ–¹æ¡ˆ...")
            
            # Fallback: ä½¿ç”¨ç®€å•çš„æ–‡æœ¬ç”Ÿæˆè€Œä¸æ˜¯structured output
            simple_prompt = f"""Based on the research topic: {research_topic}
            
Research results summary: {len(truncated_results)} sources analyzed.

Please evaluate if this research is sufficient and respond in this exact JSON format:
{{
  "is_sufficient": true,
  "knowledge_gap": "Research appears comprehensive based on available sources",
  "follow_up_queries": []
}}

Important: Respond only with valid JSON."""
            
            try:
                fallback_response = llm.invoke(simple_prompt)
                import json
                # å°è¯•è§£æJSONå“åº”
                response_text = fallback_response.content if hasattr(fallback_response, 'content') else str(fallback_response)
                # æå–JSONéƒ¨åˆ†
                import re
                json_match = re.search(r'\{.*\}', response_text, re.DOTALL)
                if json_match:
                    result_dict = json.loads(json_match.group())
                    # åˆ›å»ºReflectionå¯¹è±¡
                    result = Reflection(
                        is_sufficient=result_dict.get("is_sufficient", True),
                        knowledge_gap=result_dict.get("knowledge_gap", "Analysis completed with available data"),
                        follow_up_queries=result_dict.get("follow_up_queries", [])
                    )
                    print("âœ… Fallbackæ–¹æ¡ˆæˆåŠŸ")
                else:
                    raise ValueError("æ— æ³•è§£æJSONå“åº”")
                    
            except Exception as fallback_error:
                print(f"âŒ Fallbackæ–¹æ¡ˆä¹Ÿå¤±è´¥: {str(fallback_error)}")
                print("ğŸ›¡ï¸ ä½¿ç”¨é»˜è®¤reflectionç»“æœ")
                
                # æœ€ç»ˆfallback: åŸºäºç»“æœæ•°é‡çš„ç®€å•åˆ¤æ–­
                has_sufficient_results = len(web_research_results) >= 3
                result = Reflection(
                    is_sufficient=has_sufficient_results,
                    knowledge_gap="Analysis completed with available research data" if has_sufficient_results else "Limited research data available",
                    follow_up_queries=[] if has_sufficient_results else [f"additional information about {research_topic}"]
                )
                print(f"ğŸ›¡ï¸ é»˜è®¤åˆ¤æ–­: sufficient={has_sufficient_results}, åŸºäº{len(web_research_results)}ä¸ªæœç´¢ç»“æœ")

    except Exception as e:
        error_message = f"ReflectionèŠ‚ç‚¹å‘ç”Ÿä¸¥é‡é”™è¯¯: {str(e)}"
        print(f"ğŸ’¥ {error_message}")
        
        # ç´§æ€¥fallback: æ€»æ˜¯è®¤ä¸ºå½“å‰ç»“æœè¶³å¤Ÿï¼Œé¿å…ä¸­æ–­æµç¨‹
        result = Reflection(
            is_sufficient=True,
            knowledge_gap="Analysis completed despite technical difficulties",
            follow_up_queries=[]
        )
        print("ğŸš¨ ä½¿ç”¨ç´§æ€¥fallbackï¼Œæ ‡è®°ä¸ºsufficientä»¥ç»§ç»­æµç¨‹")

    # è¿”å›æ›´æ–°çš„çŠ¶æ€ï¼ŒåŒ…å«reflectionç»“æœ
    return {
        "research_loop_count": state["research_loop_count"],
        "reflection_is_sufficient": result.is_sufficient,  # æ–°å¢å­—æ®µä¿å­˜reflectionç»“æœ
        "reflection_knowledge_gap": result.knowledge_gap,  # æ–°å¢å­—æ®µä¿å­˜çŸ¥è¯†å·®è·
        "reflection_follow_up_queries": result.follow_up_queries,  # æ–°å¢å­—æ®µä¿å­˜follow-upæŸ¥è¯¢
        "number_of_ran_queries": len(state.get("executed_search_queries", [])),
        "plan": state.get("plan", []),
        "current_task_pointer": state.get("current_task_pointer", 0)
    }


def evaluate_research_enhanced(state: OverallState, config: RunnableConfig) -> dict:
    """
    å¢å¼ºç‰ˆç ”ç©¶è¯„ä¼°èŠ‚ç‚¹ - æ›´æ–°çŠ¶æ€ä¸­çš„è¯„ä¼°ç»“æœ
    
    è¿™ä¸ªå‡½æ•°åªè´Ÿè´£çŠ¶æ€æ›´æ–°ï¼Œä¸è´Ÿè´£è·¯ç”±å†³ç­–
    """
    configurable = Configuration.from_runnable_config(config)
    
    # è·å–reflectionç»“æœ
    research_loop_count = state.get("research_loop_count", 0)
    max_research_loops = configurable.max_research_loops
    reflection_is_sufficient = state.get("reflection_is_sufficient", False)
    reflection_follow_up_queries = state.get("reflection_follow_up_queries", [])
    
    # æ£€æŸ¥æ˜¯å¦å·²ç»å®Œæˆå¢å¼ºä»¥åŠå¢å¼ºçš„æ•ˆæœ
    enhancement_status = state.get("enhancement_status")
    enhanced_sources_count = state.get("enhanced_sources_count", 0)
    
    # æ™ºèƒ½å†³ç­–ï¼šè€ƒè™‘reflectionç»“æœå’Œå¢å¼ºæ•ˆæœ
    is_sufficient = reflection_is_sufficient
    
    # å¦‚æœreflectionè®¤ä¸ºä¸å……è¶³ï¼Œä½†æˆ‘ä»¬æˆåŠŸè¿›è¡Œäº†å†…å®¹å¢å¼ºï¼Œå¯èƒ½éœ€è¦é‡æ–°è¯„ä¼°
    if not is_sufficient and enhancement_status == "completed" and enhanced_sources_count > 0:
        print(f"ğŸ“ˆ å†…å®¹å¢å¼ºå®Œæˆ ({enhanced_sources_count} ä¸ªæº)ï¼Œæå‡å……è¶³æ€§è¯„ä¼°")
        # ç»™å¢å¼ºå†…å®¹ä¸€å®šçš„"åŠ åˆ†"
        enhancement_boost = min(enhanced_sources_count * 0.3, 0.8)
        if enhancement_boost >= 0.6:
            print(f"  âœ… åŸºäºå†…å®¹å¢å¼ºç»“æœï¼Œåˆ¤å®šä¿¡æ¯å·²å……è¶³")
            is_sufficient = True
    
    # å‡†å¤‡follow-upæŸ¥è¯¢ï¼ˆå¦‚æœéœ€è¦ç»§ç»­ç ”ç©¶ï¼‰
    follow_up_queries = reflection_follow_up_queries or []
    if not follow_up_queries and not is_sufficient:
        # å¦‚æœæ²¡æœ‰follow-upæŸ¥è¯¢ä½†ä¿¡æ¯ä¸å……è¶³ï¼Œç”Ÿæˆç®€å•çš„æŸ¥è¯¢
        plan = state.get("plan", [])
        current_pointer = state.get("current_task_pointer", 0)
        if plan and current_pointer < len(plan):
            task_description = plan[current_pointer]["description"]
            follow_up_queries = [f"more details about {task_description}"]
    
    # è®°å½•è¯„ä¼°ç»“æœåˆ°çŠ¶æ€
    final_decision = is_sufficient or research_loop_count >= max_research_loops
    
    print(f"ğŸ ç ”ç©¶è¯„ä¼°å®Œæˆ - å……è¶³æ€§: {is_sufficient}, å¾ªç¯æ¬¡æ•°: {research_loop_count}/{max_research_loops}")
    if enhancement_status == "completed":
        print(f"  ğŸ”¥ æœ¬è½®åŒ…å«Firecrawlå†…å®¹å¢å¼º: {enhanced_sources_count} ä¸ªæº")
    
    return {
        "evaluation_is_sufficient": is_sufficient,
        "evaluation_should_continue": not final_decision,
        "evaluation_follow_up_queries": follow_up_queries,
        "evaluation_research_complete": final_decision,
        "evaluation_enhancement_boost": enhanced_sources_count if enhancement_status == "completed" else 0
    }


def decide_next_research_step(state: OverallState):
    """
    æ¡ä»¶è¾¹å‡½æ•° - å†³å®šç ”ç©¶æ˜¯å¦å®Œæˆè¿˜æ˜¯ç»§ç»­
    å¯ä»¥è¿”å›å­—ç¬¦ä¸²è·¯ç”±æˆ–Sendå¯¹è±¡åˆ—è¡¨
    """
    # ä»çŠ¶æ€ä¸­è·å–è¯„ä¼°ç»“æœ
    should_continue = state.get("evaluation_should_continue", False)
    research_complete = state.get("evaluation_research_complete", False)
    
    if research_complete or not should_continue:
        print("ğŸ ç ”ç©¶æµç¨‹å®Œæˆï¼Œè®°å½•ä»»åŠ¡ç»“æœ")
        return "record_task_completion"
    else:
        print("ğŸ”„ ç»§ç»­ç ”ç©¶ï¼Œæ‰§è¡Œfollow-upæŸ¥è¯¢")
        # ç”Ÿæˆfollow-upæŸ¥è¯¢çš„Sendå¯¹è±¡
        follow_up_queries = state.get("evaluation_follow_up_queries", [])
        
        if not follow_up_queries:
            print("âš ï¸ æ²¡æœ‰follow-upæŸ¥è¯¢ï¼Œç›´æ¥å®Œæˆ")
            return "record_task_completion"
        
        # Get current task info for follow-up research
        plan = state.get("plan", [])
        current_pointer = state.get("current_task_pointer", 0)
        current_task_id = "unknown"
        
        if plan and current_pointer < len(plan):
            current_task_id = plan[current_pointer]["id"]
        
        print(f"ğŸ”„ ç”Ÿæˆ {len(follow_up_queries)} ä¸ªfollow-upæŸ¥è¯¢")
        
        # è¿”å›follow-upæŸ¥è¯¢çš„Sendåˆ—è¡¨
        from langgraph.types import Send
        return [
            Send(
                "web_research",
                {
                    "search_query": follow_up_query,
                    "id": state.get("number_of_ran_queries", 0) + int(idx),
                    "current_task_id": current_task_id
                },
            )
            for idx, follow_up_query in enumerate(follow_up_queries)
        ]


def finalize_answer(state: OverallState, config: RunnableConfig) -> dict:
    """Generate the final research report by synthesizing all task findings, using batch generation for detailed content."""
    try:
        configurable = Configuration.from_runnable_config(config)
        llm = ChatGoogleGenerativeAI(
            model=configurable.reflection_model,
            temperature=0.3,
            max_retries=2,
            api_key=os.getenv("GEMINI_API_KEY"),
        )
        
        plan = state.get("plan", [])
        if not plan:
            return {
                "messages": [AIMessage(content="No research plan available to generate report")],
                "final_report_markdown": "No research plan available to generate report"
            }
        
        # Build ledger map and task results map
        ledger = state.get("ledger", [])
        ledger_map = {entry["task_id"]: entry for entry in ledger}
        
        task_specific_results = state.get("task_specific_results", [])
        task_results_map = {}
        for result in task_specific_results:
            task_id = result.get("task_id")
            if task_id:
                if task_id not in task_results_map:
                    task_results_map[task_id] = []
                task_results_map[task_id].append(result)
        
        # Build source mapping for citation conversion
        sources_gathered = state.get("sources_gathered", [])
        source_mapping = build_source_mapping(sources_gathered)
        
        report_sections = []
        
        # Introduction
        intro_prompt = f"""As a Senior Research Analyst at a leading global consultancy, write a professional Executive Summary for this research report.

Research Topic: {state.get('user_query', 'Research Topic')}

EXECUTIVE SUMMARY REQUIREMENTS:
- **Strategic Context**: Establish the business importance and relevance of this research
- **Key Market Dynamics**: Highlight the most critical trends and drivers shaping this space
- **Core Insights**: Summarize the 3-4 most significant findings that executives need to know
- **Strategic Implications**: What this means for business leaders, investors, and policymakers
- **Market Opportunity**: Size the opportunity and highlight key growth drivers

WRITING STYLE:
- Executive-level language: professional, authoritative, accessible
- Lead with business impact and strategic significance
- Include specific data points that demonstrate market scale and momentum
- Focus on actionable insights rather than academic abstractions
- 3-4 well-structured paragraphs

EXAMPLE OPENING: "The global [market/sector] is experiencing unprecedented transformation, driven by [key factors]. With market valuations reaching $X billion and projected growth of Y%, this represents a critical inflection point for [stakeholder groups]..."

IMPORTANT: Write only the Executive Summary content. No meta-commentary, no section headers."""

        try:
            introduction = llm.invoke(intro_prompt).content
            introduction = clean_generated_content(introduction)
            report_sections.append(f"# {state.get('user_query', 'Research Report')}\n\n## Executive Summary\n\n{introduction}\n")
        except Exception as e:
            report_sections.append(f"# {state.get('user_query', 'Research Report')}\n\n## Executive Summary\n\n*Executive Summary generation encountered technical issues. Report continues with detailed analysis.*\n")

        # Generate sections for each task
        for task in plan:
            task_id = task["id"]
            task_description = task["description"]
            ledger_entry = ledger_map.get(task_id)
            if not ledger_entry:
                report_sections.append(f"## {task_description}\n\n*Analysis pending - comprehensive data collection in progress.*\n")
                continue
            
            # Get task-specific results first, then fall back to web_research_result if empty
            task_results = task_results_map.get(task_id, [])
            detailed_contents = [result["content"] for result in task_results]
            
            # Fallback: if no task-specific results, use all web_research_result as content
            if not detailed_contents:
                web_research_result = state.get("web_research_result", [])
                detailed_contents = web_research_result
                print(f"Warning: No task-specific results for {task_id}, using fallback web_research_result with {len(detailed_contents)} items")
            
            if not detailed_contents:
                # If still no content, create a section with just the summary
                section_content = ledger_entry['findings_summary']
                report_sections.append(f"## {task_description}\n\n{section_content}\n")
                continue
            
            batches = split_by_tokens(detailed_contents, max_tokens=150000)  # Increased token limit
            section_content = ""
            previous_content = ""
            
            for i, batch in enumerate(batches):
                is_last = (i == len(batches) - 1)
                batched_content = "\n\n".join(batch)
                
                # Convert citations to readable format
                batched_content = convert_citations_to_readable(batched_content, source_mapping)
                
                section_prompt = f"""As a Senior Research Analyst at a leading global consultancy, synthesize these research findings into a professional analysis section.

SECTION FOCUS: {task_description}
STRATEGIC CONTEXT: {ledger_entry['findings_summary']}

RESEARCH DATA:
{batched_content}

PROFESSIONAL ANALYSIS REQUIREMENTS:
1. **Market Intelligence**: Present data within strategic business context
2. **Competitive Landscape**: Highlight key players, market dynamics, and positioning
3. **Technology Trends**: Identify innovation drivers and disruptive forces
4. **Implementation Insights**: Showcase real-world case studies and best practices  
5. **Business Implications**: Connect findings to strategic decision-making
6. **Risk Assessment**: Identify challenges, barriers, and mitigation strategies

WRITING STANDARDS:
- Lead with executive insights, support with data
- Transform raw information into strategic intelligence
- Use professional attribution: "According to industry analysis from [source]..." 
- Include specific metrics that demonstrate scale and trajectory
- Organize with clear subheadings for easy navigation
- Prioritize actionable insights over academic detail

STRUCTURE GUIDELINES:
- **Market Overview**: Size, growth, key dynamics
- **Technology Analysis**: Current capabilities and emerging innovations  
- **Case Studies**: Real-world implementations and lessons learned
- **Strategic Implications**: What this means for market participants

CITATION APPROACH:
- Integrate sources naturally: "Research from McKinsey indicates..."
- Provide credible context: "According to government data released in 2024..."
- Emphasize authoritative sources: major consulting firms, industry associations, government agencies

OUTPUT REQUIREMENTS:
- Professional consulting report section
- Clear strategic narrative with supporting evidence
- Executive-appropriate language and insights
- Logical flow from analysis to business implications"""

                if previous_content:
                    section_prompt += f"\n\nBUILD UPON PREVIOUS ANALYSIS:\n{previous_content}\n\nContinue the strategic narrative, avoiding redundancy while building comprehensive coverage."
                
                if is_last:
                    section_prompt += "\n\nCONCLUSION: Synthesize this section with strategic implications and key takeaways for executives."
                else:
                    section_prompt += "\n\nCONTINUATION: Develop this analysis further - more detailed findings follow."

                section_prompt += "\n\nIMPORTANT: Output professional analysis content only. No meta-commentary or process notes."

                try:
                    batch_content = llm.invoke(section_prompt).content
                    
                    # Enhanced content cleaning
                    batch_content = clean_generated_content(batch_content)
                    batch_content = remove_prompt_remnants(batch_content)
                    
                    section_content += batch_content + "\n"
                    previous_content = section_content
                except Exception as e:
                    section_content += f"*Error generating batch content: {str(e)}*\n"
            
            report_sections.append(f"## {task_description}\n\n{section_content}\n")

        # Conclusion
        conclusion_prompt = f"""As a Senior Research Analyst at a leading global consultancy, write a comprehensive Strategic Implications & Recommendations section for this research report.

RESEARCH TOPIC: {state.get('user_query', 'Not specified')}

KEY FINDINGS SUMMARY:
{chr(10).join([f"- {task['description']}: {ledger_map.get(task['id'], {}).get('findings_summary', 'Analysis in progress')}" for task in plan])}

STRATEGIC IMPLICATIONS REQUIREMENTS:
1. **Market Trajectory**: Where is this industry/sector heading? What are the key inflection points?
2. **Investment Thesis**: What opportunities present the highest return potential?
3. **Strategic Priorities**: What should business leaders prioritize in the next 12-24 months?
4. **Risk Mitigation**: What are the primary risks and how can they be managed?
5. **Competitive Advantage**: How can organizations position themselves to win?

RECOMMENDATIONS FRAMEWORK:
- **Immediate Actions** (0-6 months): Tactical steps for quick wins
- **Strategic Initiatives** (6-18 months): Medium-term capability building  
- **Long-term Positioning** (18+ months): Future market preparation

EXECUTIVE COMMUNICATION STYLE:
- Lead with business impact and competitive implications
- Provide specific, actionable recommendations with clear rationale
- Include investment and resource allocation guidance
- Address both opportunities and risks with balanced perspective
- Use authoritative, confident language appropriate for C-suite audience

CONCLUSION STRUCTURE:
1. **Strategic Synthesis**: Connect findings to broader market dynamics
2. **Key Recommendations**: 3-4 priority actions with business rationale
3. **Future Outlook**: Market evolution and emerging opportunities
4. **Next Steps**: Specific actions for continued competitive advantage

IMPORTANT: Write as a senior consultant presenting to executive leadership. Focus on strategic implications and actionable intelligence rather than academic conclusions."""

        try:
            conclusion = llm.invoke(conclusion_prompt).content
            conclusion = clean_generated_content(conclusion)
            report_sections.append(f"## Strategic Implications & Recommendations\n\n{conclusion}\n")
        except Exception as e:
            report_sections.append(f"## Strategic Implications & Recommendations\n\n*Strategic analysis and recommendations section is being finalized to provide executive-level insights and actionable guidance.*\n")

        # Assemble final report
        final_report_markdown = "\n\n---\n\n".join(report_sections)
        
        # Final quality check and cleaning
        final_report_markdown = final_quality_check(final_report_markdown)
        
        return {
            "messages": [AIMessage(content=final_report_markdown)],
            "final_report_markdown": final_report_markdown
        }
    except Exception as e:
        error_message = f"Error generating final report: {str(e)}"
        return {
            "messages": [AIMessage(content=error_message)],
            "final_report_markdown": error_message
        }

def build_source_mapping(sources_gathered):
    """æ„å»ºæºæ–‡ä»¶æ˜ å°„ï¼Œç”¨äºå¼•ç”¨è½¬æ¢"""
    mapping = {}
    for i, source in enumerate(sources_gathered):
        # Extract domain from URL for readable citation
        domain = extract_domain(source.get("value", ""))
        label = source.get("label", domain)
        
        # Create mapping for different citation formats
        short_url = source.get("short_url", "")
        if short_url:
            # Extract ID from short URL
            import re
            id_match = re.search(r'/id/([^/]+)', short_url)
            if id_match:
                citation_id = id_match.group(1)
                mapping[citation_id] = {
                    "label": label,
                    "domain": domain,
                    "value": source.get("value", "")
                }
    return mapping

def extract_domain(url):
    """ä»URLä¸­æå–åŸŸå"""
    import re
    if not url:
        return "Unknown"
    
    # Extract domain from URL
    domain_match = re.search(r'https?://(?:www\.)?([^/]+)', url)
    if domain_match:
        domain = domain_match.group(1)
        # Simplify common domains
        if "google.com" in domain:
            return "Google"
        elif "wikipedia" in domain:
            return "Wikipedia" 
        elif "youtube" in domain:
            return "YouTube"
        else:
            return domain.split('.')[0].title()
    return "Web Source"

def convert_citations_to_readable(content, source_mapping):
    """å°†åŸå§‹å¼•ç”¨æ ‡è®°è½¬æ¢ä¸ºå¯è¯»çš„å¼•ç”¨æ ¼å¼"""
    import re
    
    def replace_citation(match):
        citation_id = match.group(1)
        if citation_id in source_mapping:
            source_info = source_mapping[citation_id]
            return f"[Source: {source_info['domain']}]"
        return ""
    
    # Convert Vertex AI citations
    content = re.sub(r'\[vertexaisearch\.cloud\.google\.com/id/([^\]]+)\]', 
                     replace_citation, content)
    
    # Convert other citation formats
    content = re.sub(r'\[([a-z0-9\-]+)\]', r'[Source: \1]', content)
    
    return content

def clean_generated_content(content):
    """æ¸…ç†ç”Ÿæˆå†…å®¹ä¸­çš„å…ƒæ–‡æœ¬å’Œæ— å…³ä¿¡æ¯"""
    if not content:
        return content
    
    # Remove common meta-text at beginning
    meta_prefixes = [
        "here is", "this is", "based on", "according to", "å¥½çš„", "æ ¹æ®",
        "ä»¥ä¸‹æ˜¯", "here's", "below is", "following is"
    ]
    
    lines = content.split('\n')
    cleaned_lines = []
    
    for line in lines:
        line = line.strip()
        if line:
            # Skip lines that start with meta-text
            line_lower = line.lower()
            is_meta = any(line_lower.startswith(prefix) for prefix in meta_prefixes)
            if not is_meta:
                cleaned_lines.append(line)
    
    return '\n'.join(cleaned_lines)

def remove_prompt_remnants(content):
    """ç§»é™¤å†…å®¹ä¸­çš„Promptæ®‹ç•™"""
    import re
    
    # Remove instruction-like text
    content = re.sub(r'INSTRUCTIONS?:.*?(?=\n\n|\n[A-Z]|\Z)', '', content, flags=re.DOTALL | re.IGNORECASE)
    content = re.sub(r'REQUIREMENTS?:.*?(?=\n\n|\n[A-Z]|\Z)', '', content, flags=re.DOTALL | re.IGNORECASE)
    content = re.sub(r'IMPORTANT:.*?(?=\n\n|\n[A-Z]|\Z)', '', content, flags=re.DOTALL | re.IGNORECASE)
    
    # Remove standalone bullets or dashes
    content = re.sub(r'^\s*[-â€¢]\s*$', '', content, flags=re.MULTILINE)
    
    # Remove multiple consecutive line breaks
    content = re.sub(r'\n{3,}', '\n\n', content)
    
    return content.strip()

def final_quality_check(content):
    """æœ€ç»ˆè´¨é‡æ£€æŸ¥å’Œæ¸…ç†"""
    # Remove any remaining citation URLs
    import re
    content = re.sub(r'https?://[^\s\]]+', '', content)
    
    # Fix spacing issues
    content = re.sub(r'\n{3,}', '\n\n', content)
    content = re.sub(r'[ \t]+', ' ', content)
    
    # Remove standalone punctuation lines
    content = re.sub(r'^\s*[-.â€¢]+\s*$', '', content, flags=re.MULTILINE)
    
    # Ensure proper spacing around headers
    content = re.sub(r'\n(#+[^\n]+)\n', r'\n\n\1\n\n', content)
    
    return content.strip()


def planner_node(state: OverallState, config: RunnableConfig) -> dict:
    """LangGraph node that generates a multi-step research plan based on the user's question."""
    configurable = Configuration.from_runnable_config(config)
    llm = ChatGoogleGenerativeAI(
        model=configurable.query_generator_model,
        temperature=0.7,
        max_retries=2,
        api_key=os.getenv("GEMINI_API_KEY"),
    )
    structured_llm = llm.with_structured_output(ResearchPlan)

    # Get user query, prioritize from user_query, fallback to messages
    user_query = state.get("user_query") or get_research_topic(state["messages"])
    
    # Use centrally managed planning prompt
    formatted_prompt = planning_instructions.format(user_query=user_query)
    
    try:
        result = structured_llm.invoke(formatted_prompt)
        # Convert ResearchPlan to expected format
        plan = [{"id": task.id, "description": task.description, "info_needed": True, "source_hint": task.description, "status": "pending"} for task in result.tasks]
        
        return {
            "user_query": user_query,
            "plan": plan,
            "current_task_pointer": 0
        }
    except Exception as e:
        print(f"Planning failed: {e}")
        # Provide default single-task plan as fallback
        return {
            "user_query": user_query,
            "plan": [{"id": "task-1", "description": f"Research and answer: {user_query}", "info_needed": True, "source_hint": user_query, "status": "pending"}],
            "current_task_pointer": 0
        }


def record_task_completion_node(state: OverallState, config: RunnableConfig) -> dict:
    """Record the findings for the current task and prepare for the next task."""
    try:
        # Get current task info
        plan = state.get("plan", [])
        current_pointer = state.get("current_task_pointer", 0)
        
        if not plan or current_pointer >= len(plan):
            return {
                "messages": [AIMessage(content="Error: Invalid task pointer or empty plan")],
                "next_node_decision": "end"
            }
            
        current_task = plan[current_pointer]
        current_task_id = current_task.get("id")
        
        # Get detailed findings for current task
        detailed_findings = state.get("current_task_detailed_findings", [])
        task_specific_findings = [
            finding["content"] for finding in detailed_findings 
            if finding.get("task_id") == current_task_id
        ]
        
        # If no task-specific findings found, try to get recent web results as fallback
        if not task_specific_findings:
            print(f"Warning: No task-specific findings found for task {current_task_id}, using recent web results as fallback")
            web_results = state.get("web_research_result", [])
            # Take the most recent results (assume they belong to current task)
            task_specific_findings = web_results[-3:] if len(web_results) > 3 else web_results
        
        # Generate task summary
        task_summary = _summarize_task_findings(
            current_task["description"],
            task_specific_findings,
            config
        )
        
        # Create citations from detailed findings
        citations_for_snippets = []
        for finding in detailed_findings:
            if finding.get("task_id") == current_task_id and finding.get("source"):
                citations_for_snippets.append({
                    "snippet": finding["content"],
                    "source": str(finding["source"])
                })
        
        # Create ledger entry with detailed findings
        ledger_entry = {
            "task_id": current_task_id,
            "description": current_task["description"],
            "findings_summary": task_summary,
            "detailed_snippets": task_specific_findings,
            "citations_for_snippets": citations_for_snippets
        }
        
        # Update plan status
        plan[current_pointer]["status"] = "completed"
        
        # Clear current task findings to prepare for next task
        return {
            "ledger": [ledger_entry],
            "global_summary_memory": [task_summary],
            "plan": plan,
            "current_task_pointer": current_pointer + 1,
            "current_task_detailed_findings": [],  # Clear for next task
            "next_node_decision": "continue" if current_pointer + 1 < len(plan) else "end"
        }
    except Exception as e:
        error_message = f"Error in record_task_completion_node: {str(e)}"
        print(error_message)
        return {
            "messages": [AIMessage(content=error_message)],
            "next_node_decision": "end"
        }


def _summarize_task_findings(task_description: str, web_results: List[str], config: RunnableConfig) -> str:
    """Helper function to summarize web research results for a specific task."""
    if not web_results:
        return f"No specific findings available for task: {task_description}"
    
    # Use recent results (last 3 entries) to avoid overwhelming context
    recent_results = web_results[-3:] if len(web_results) > 3 else web_results
    context_to_summarize = "\n---\n".join(recent_results)
    
    configurable = Configuration.from_runnable_config(config)
    llm = ChatGoogleGenerativeAI(
        model=configurable.reflection_model,
        temperature=0.3,
        max_retries=2,
        api_key=os.getenv("GEMINI_API_KEY"),
    )
    
    prompt = f"""Given the research task: "{task_description}"

And the following research findings:
{context_to_summarize}

Please provide a concise summary (1-2 sentences) of the key findings that directly address this specific task.

Task Summary:"""
    
    try:
        response = llm.invoke(prompt)
        return response.content if hasattr(response, 'content') else str(response)
    except Exception as e:
        print(f"Task summarization failed: {e}")
        return f"Completed research for: {task_description}"


def decide_next_step_in_plan(state: OverallState) -> str:
    """Conditional edge function that determines whether to continue with next task or finalize."""
    current_pointer = state.get("current_task_pointer", 0)
    plan = state.get("plan", [])
    
    if current_pointer < len(plan):
        print(f"--- Moving to next task (pointer: {current_pointer}) ---")
        return "generate_query"
    else:
        print("--- All tasks completed. Finalizing answer ---")
        return "finalize_answer"


# Create our Agent Graph
builder = StateGraph(OverallState, config_schema=Configuration)

# Define the nodes we will cycle between
builder.add_node("planner", planner_node)
builder.add_node("generate_query", generate_query)
builder.add_node("web_research", web_research)
builder.add_node("reflection", reflection)
builder.add_node("content_enhancement", content_enhancement_analysis)  # æ–°å¢å†…å®¹å¢å¼ºèŠ‚ç‚¹
builder.add_node("evaluate_research_enhanced", evaluate_research_enhanced)  # æ–°å¢å¢å¼ºç‰ˆè¯„ä¼°èŠ‚ç‚¹
builder.add_node("record_task_completion", record_task_completion_node)  # New node for Day 2
builder.add_node("finalize_answer", finalize_answer)

# Set the entrypoint as `planner`
builder.add_edge(START, "planner")
builder.add_edge("planner", "generate_query")

# Add conditional edge to continue with search queries in a parallel branch
builder.add_conditional_edges(
    "generate_query", continue_to_web_research, ["web_research"]
)

# Reflect on the web research
builder.add_edge("web_research", "reflection")

# ä¿®æ”¹reflectionåçš„è·¯ç”±é€»è¾‘ - æ·»åŠ æ™ºèƒ½å†…å®¹å¢å¼ºåˆ¤æ–­
builder.add_conditional_edges(
    "reflection", 
    should_enhance_content, 
    {
        "analyze_enhancement_need": "content_enhancement",
        "continue_without_enhancement": "evaluate_research_enhanced"
    }
)

# å†…å®¹å¢å¼ºå®Œæˆåè¿›å…¥è¯„ä¼°é˜¶æ®µ
builder.add_edge("content_enhancement", "evaluate_research_enhanced")

# è¯„ä¼°å®Œæˆåå†³å®šä¸‹ä¸€æ­¥ - ç»§ç»­ç ”ç©¶æˆ–å®Œæˆä»»åŠ¡
builder.add_conditional_edges(
    "evaluate_research_enhanced", 
    decide_next_research_step, 
    ["web_research", "record_task_completion"]  # å¯ä»¥è·¯ç”±åˆ°è¿™ä¸¤ä¸ªç›®æ ‡
)

# å½“decide_next_research_stepè¿”å›"continue_research"æ—¶ï¼Œä½¿ç”¨follow-upæŸ¥è¯¢
# è¿™å°†é€šè¿‡continue_research_with_followupå‡½æ•°ç”Ÿæˆæ–°çš„web_researchä»»åŠ¡

# After recording task completion, decide next step in plan (multi-task loop)
builder.add_conditional_edges(
    "record_task_completion", 
    decide_next_step_in_plan, 
    ["generate_query", "finalize_answer"]
)

# Finalize the answer
builder.add_edge("finalize_answer", END)

graph = builder.compile(name="pro-search-agent")

def split_by_tokens(texts, max_tokens=150000, encoding_name="cl100k_base"):
    """æ™ºèƒ½åˆ†æ‰¹å¤„ç†æ–‡æœ¬ï¼Œä¿ç•™é‡è¦ä¸Šä¸‹æ–‡å’Œä¿¡æ¯å®Œæ•´æ€§"""
    try:
        encoding = tiktoken.get_encoding(encoding_name)
    except ImportError:
        # Fallback to simple character-based estimation
        return simple_split_by_chars(texts, max_tokens * 4)  # Rough estimation: 4 chars per token
    
    batches = []
    current_batch = []
    current_tokens = 0
    
    for text in texts:
        if not text:
            continue
            
        text_tokens = len(encoding.encode(str(text)))
        
        # If single text is too large, intelligently extract key sections
        if text_tokens > max_tokens * 0.8:
            text = extract_key_sections(text, max_tokens * 0.7, encoding)
            text_tokens = len(encoding.encode(str(text)))
        
        # Check if adding this text would exceed the limit
        if current_tokens + text_tokens > max_tokens and current_batch:
            # Finalize current batch
            batches.append(current_batch)
            current_batch = [text]
            current_tokens = text_tokens
        else:
            current_batch.append(text)
            current_tokens += text_tokens
    
    # Add the last batch if it has content
    if current_batch:
        batches.append(current_batch)
    
    return batches

def extract_key_sections(content, max_tokens, encoding):
    """ä»é•¿å†…å®¹ä¸­æ™ºèƒ½æå–å…³é”®éƒ¨åˆ†ï¼Œä¼˜å…ˆä¿ç•™é‡è¦ä¿¡æ¯"""
    if not content:
        return content
    
    # Split content into sections
    sections = content.split('\n\n')
    key_sections = []
    tokens_used = 0
    priority_sections = []
    regular_sections = []
    
    # Categorize sections by importance
    for section in sections:
        if is_factual_section(section):
            priority_sections.append(section)
        else:
            regular_sections.append(section)
    
    # Add priority sections first
    for section in priority_sections:
        section_tokens = len(encoding.encode(section))
        if tokens_used + section_tokens <= max_tokens:
            key_sections.append(section)
            tokens_used += section_tokens
        elif is_critical_section(section):
            # For critical sections, truncate but include
            truncated = truncate_section(section, max_tokens - tokens_used, encoding)
            if truncated:
                key_sections.append(truncated)
            break
    
    # Add regular sections if space allows
    for section in regular_sections:
        section_tokens = len(encoding.encode(section))
        if tokens_used + section_tokens <= max_tokens:
            key_sections.append(section)
            tokens_used += section_tokens
        else:
            break
    
    return '\n\n'.join(key_sections)

def is_factual_section(section):
    """åˆ¤æ–­æ®µè½æ˜¯å¦åŒ…å«é‡è¦äº‹å®ä¿¡æ¯"""
    factual_indicators = [
        r'\d{4}',  # Years
        r'\$[\d,]+',  # Money amounts
        r'\d+%',  # Percentages
        r'\d+\.?\d*\s*(million|billion|thousand)',  # Large numbers
        r'(acquired|purchased|bought|sold)',  # Business actions
        r'(announced|launched|released)',  # Event verbs
        r'[A-Z][a-z]+\s+(Inc|Corp|Ltd|Company)',  # Company names
    ]
    
    import re
    for pattern in factual_indicators:
        if re.search(pattern, section, re.IGNORECASE):
            return True
    return False

def is_critical_section(section):
    """åˆ¤æ–­æ˜¯å¦ä¸ºå…³é”®æ®µè½ï¼ˆå³ä½¿è¶…é•¿ä¹Ÿè¦ä¿ç•™ï¼‰"""
    critical_keywords = [
        'acquisition', 'merger', 'financial', 'revenue', 'profit',
        'strategy', 'impact', 'result', 'conclusion', 'summary'
    ]
    
    section_lower = section.lower()
    return any(keyword in section_lower for keyword in critical_keywords)

def truncate_section(section, max_tokens, encoding):
    """æ™ºèƒ½æˆªå–æ®µè½ï¼Œä¿ç•™æœ€é‡è¦çš„éƒ¨åˆ†"""
    if not section:
        return ""
    
    sentences = section.split('. ')
    truncated_sentences = []
    tokens_used = 0
    
    for sentence in sentences:
        sentence_tokens = len(encoding.encode(sentence))
        if tokens_used + sentence_tokens <= max_tokens:
            truncated_sentences.append(sentence)
            tokens_used += sentence_tokens
        else:
            break
    
    result = '. '.join(truncated_sentences)
    if result and not result.endswith('.'):
        result += '.'
    
    return result

def simple_split_by_chars(texts, max_chars):
    """å­—ç¬¦çº§åˆ«çš„ç®€å•åˆ†æ‰¹ï¼ˆå¤‡ç”¨æ–¹æ¡ˆï¼‰"""
    batches = []
    current_batch = []
    current_chars = 0
    
    for text in texts:
        text_chars = len(str(text))
        if current_chars + text_chars > max_chars and current_batch:
            batches.append(current_batch)
            current_batch = [text]
            current_chars = text_chars
        else:
            current_batch.append(text)
            current_chars += text_chars
    
    if current_batch:
        batches.append(current_batch)
    
    return batches
