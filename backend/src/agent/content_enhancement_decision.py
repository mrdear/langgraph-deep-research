"""
æ™ºèƒ½å†…å®¹å¢å¼ºå†³ç­–æ¨¡å— - å†³å®šä½•æ—¶ä½¿ç”¨Firecrawlè¿›è¡Œæ·±åº¦å†…å®¹æŠ“å–
"""

import os
from typing import Dict, List, Any, Optional
from dataclasses import dataclass
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain_core.runnables import RunnableConfig
from firecrawl import FirecrawlApp

@dataclass
class EnhancementDecision:
    """å†…å®¹å¢å¼ºå†³ç­–ç»“æœ"""
    needs_enhancement: bool
    priority_urls: List[Dict[str, Any]]
    reasoning: str
    confidence_score: float  # 0-1
    enhancement_type: str  # "none", "selective", "comprehensive"


class ContentEnhancementDecisionMaker:
    """æ™ºèƒ½å†…å®¹å¢å¼ºå†³ç­–å™¨ - ç±»ä¼¼reflectionæœºåˆ¶"""
    
    def __init__(self):
        self.firecrawl_app = None
        if os.getenv("FIRECRAWL_API_KEY"):
            self.firecrawl_app = FirecrawlApp(api_key=os.getenv("FIRECRAWL_API_KEY"))
    
    def analyze_enhancement_need(
        self, 
        research_topic: str,
        current_findings: List[str],
        grounding_sources: List[Dict[str, Any]],
        config: RunnableConfig
    ) -> EnhancementDecision:
        """
        æ™ºèƒ½åˆ†ææ˜¯å¦éœ€è¦å†…å®¹å¢å¼º - ä½¿ç”¨LLMåšåˆ¤æ–­
        
        ç±»ä¼¼reflectionæœºåˆ¶ï¼Œè®©LLMåˆ†æå½“å‰ç ”ç©¶è´¨é‡å¹¶å†³å®šæ˜¯å¦éœ€è¦æ·±åº¦æŠ“å–
        """
        
        # æ„å»ºåˆ†ææç¤ºè¯
        analysis_prompt = self._build_analysis_prompt(
            research_topic, current_findings, grounding_sources
        )
        
        # ä½¿ç”¨LLMè¿›è¡Œæ™ºèƒ½åˆ¤æ–­
        from agent.configuration import Configuration
        configurable = Configuration.from_runnable_config(config)
        
        llm = ChatGoogleGenerativeAI(
            model=configurable.reflection_model,  # ä½¿ç”¨å’Œreflectionç›¸åŒçš„æ¨¡å‹
            temperature=0.3,  # ä½æ¸©åº¦ç¡®ä¿ä¸€è‡´æ€§
            max_retries=2,
            api_key=os.getenv("GEMINI_API_KEY"),
        )
        
        response = llm.invoke(analysis_prompt)
        decision_text = response.content if hasattr(response, 'content') else str(response)
        
        # è§£æLLMçš„å†³ç­–
        return self._parse_llm_decision(decision_text, grounding_sources)
    
    def _build_analysis_prompt(
        self, 
        research_topic: str, 
        current_findings: List[str], 
        grounding_sources: List[Dict[str, Any]]
    ) -> str:
        """æ„å»ºåˆ†ææç¤ºè¯"""
        
        findings_summary = "\n---\n".join(current_findings[-3:])  # æœ€è¿‘3ä¸ªç»“æœ
        
        sources_list = "\n".join([
            f"- {source.get('title', 'N/A')}: {source.get('url', 'N/A')}"
            for source in grounding_sources[:5]  # å‰5ä¸ªæº
        ])
        
        return f"""ä½ æ˜¯ä¸€ä¸ªç ”ç©¶è´¨é‡è¯„ä¼°ä¸“å®¶ã€‚è¯·åˆ†æå½“å‰çš„ç ”ç©¶ç»“æœè´¨é‡ï¼Œå¹¶å†³å®šæ˜¯å¦éœ€è¦æ·±åº¦å†…å®¹å¢å¼ºã€‚

ç ”ç©¶ä¸»é¢˜: {research_topic}

å½“å‰ç ”ç©¶å‘ç°:
{findings_summary}

å¯ç”¨çš„ä¿¡æ¯æº:
{sources_list}

è¯·æ ¹æ®ä»¥ä¸‹æ ‡å‡†è¿›è¡Œè¯„ä¼°:

1. **å†…å®¹æ·±åº¦ä¸è¶³çš„ä¿¡å·**:
   - ç¼ºä¹å…·ä½“æ•°æ®ã€ç»Ÿè®¡ä¿¡æ¯ã€æ¡ˆä¾‹ç ”ç©¶
   - æè¿°è¿‡äºæ³›æ³›ï¼Œç¼ºä¹æŠ€æœ¯ç»†èŠ‚
   - æ²¡æœ‰æåŠé‡è¦çš„å…¬å¸ã€é¡¹ç›®æˆ–å®æ–½æ¡ˆä¾‹
   - ä¿¡æ¯æºè´¨é‡ä¸é«˜ï¼ˆéæƒå¨ç½‘ç«™ï¼‰

2. **éœ€è¦æ·±åº¦æŠ“å–çš„æƒ…å†µ**:
   - ç ”ç©¶ä¸»é¢˜éœ€è¦è¯¦ç»†çš„æŠ€æœ¯è¯´æ˜
   - å½“å‰ç»“æœç¼ºä¹å…³é”®æ•°æ®æ”¯æ’‘
   - å­˜åœ¨æƒå¨ä¿¡æ¯æºä½†å†…å®¹è¢«æˆªæ–­
   - éœ€è¦è·å–å®Œæ•´çš„æŠ¥å‘Šæˆ–ç ”ç©¶å†…å®¹

3. **è¯„ä¼°å½“å‰ä¿¡æ¯æºçš„ä»·å€¼**:
   - å®˜æ–¹ç½‘ç«™/æ–‡æ¡£: é«˜ä»·å€¼
   - å­¦æœ¯è®ºæ–‡/ç ”ç©¶æŠ¥å‘Š: é«˜ä»·å€¼  
   - ç»´åŸºç™¾ç§‘/ç™¾ç§‘ç±»: ä¸­ç­‰ä»·å€¼
   - æ–°é—»æŠ¥é“: æ ¹æ®è¯¦ç»†ç¨‹åº¦åˆ¤æ–­
   - åšå®¢/è®ºå›: ä½ä»·å€¼

è¯·æŒ‰ä»¥ä¸‹æ ¼å¼å›ç­”:

**å†³ç­–**: [ENHANCE/NO_ENHANCE]
**ç½®ä¿¡åº¦**: [0.1-1.0]
**å¢å¼ºç±»å‹**: [selective/comprehensive/none]
**æ¨èURLæ•°é‡**: [0-3]
**æ¨ç†è¿‡ç¨‹**: 
[è¯¦ç»†è¯´æ˜ä½ çš„åˆ¤æ–­ç†ç”±ï¼ŒåŒ…æ‹¬å½“å‰å†…å®¹çš„ä¸è¶³ä¹‹å¤„å’Œé¢„æœŸçš„æ”¹è¿›æ•ˆæœ]

**ä¼˜å…ˆURLs** (å¦‚æœéœ€è¦å¢å¼º):
[ä»ä¿¡æ¯æºä¸­é€‰æ‹©æœ€å€¼å¾—æ·±åº¦æŠ“å–çš„URLï¼ŒæŒ‰ä¼˜å…ˆçº§æ’åº]
"""

    def _parse_llm_decision(
        self, 
        decision_text: str, 
        grounding_sources: List[Dict[str, Any]]
    ) -> EnhancementDecision:
        """è§£æLLMçš„å†³ç­–ç»“æœ"""
        
        decision_text = decision_text.lower()
        
        # è§£æåŸºæœ¬å†³ç­–
        needs_enhancement = "enhance" in decision_text and "no_enhance" not in decision_text
        
        # è§£æç½®ä¿¡åº¦
        confidence_score = 0.5  # é»˜è®¤å€¼
        import re
        confidence_match = re.search(r'ç½®ä¿¡åº¦.*?([0-9]\.[0-9])', decision_text)
        if confidence_match:
            try:
                confidence_score = float(confidence_match.group(1))
            except:
                pass
        
        # è§£æå¢å¼ºç±»å‹
        enhancement_type = "none"
        if "selective" in decision_text:
            enhancement_type = "selective"
        elif "comprehensive" in decision_text:
            enhancement_type = "comprehensive"
        elif needs_enhancement:
            enhancement_type = "selective"  # é»˜è®¤é€‰æ‹©æ€§å¢å¼º
        
        # é€‰æ‹©ä¼˜å…ˆURLï¼ˆç®€åŒ–ç‰ˆæœ¬ï¼Œå¯ä»¥åç»­æ”¹è¿›ä¸ºLLMé€‰æ‹©ï¼‰
        priority_urls = []
        if needs_enhancement and grounding_sources:
            # ç®€å•çš„ä¼˜å…ˆçº§ç®—æ³•
            scored_sources = []
            for source in grounding_sources:
                score = self._calculate_url_priority(source)
                scored_sources.append((source, score))
            
            # æŒ‰è¯„åˆ†æ’åºï¼Œé€‰æ‹©å‰2-3ä¸ª
            scored_sources.sort(key=lambda x: x[1], reverse=True)
            max_urls = 3 if enhancement_type == "comprehensive" else 2
            
            priority_urls = [
                {
                    "title": source.get("title", ""),
                    "url": source.get("url", ""),
                    "priority_score": score,
                    "reasoning": f"è¯„åˆ†: {score:.2f}"
                }
                for source, score in scored_sources[:max_urls]
                if score > 0.3  # åªé€‰æ‹©è¯„åˆ†è¾ƒé«˜çš„
            ]
        
        return EnhancementDecision(
            needs_enhancement=needs_enhancement,
            priority_urls=priority_urls,
            reasoning=decision_text,
            confidence_score=confidence_score,
            enhancement_type=enhancement_type
        )
    
    def _calculate_url_priority(self, source: Dict[str, Any]) -> float:
        """è®¡ç®—URLçš„ä¼˜å…ˆçº§è¯„åˆ†"""
        score = 0.0
        
        url = source.get("url", "").lower()
        title = source.get("title", "").lower()
        
        # å®˜æ–¹ç½‘ç«™å’Œæ–‡æ¡£
        if any(domain in url for domain in [".gov", ".edu", ".org"]):
            score += 0.4
        
        # çŸ¥åå¹³å°
        if any(platform in url for platform in ["wikipedia", "arxiv", "ieee", "acm"]):
            score += 0.3
        
        # æŠ€æœ¯å†…å®¹æŒ‡æ ‡
        if any(keyword in title for keyword in ["report", "study", "research", "analysis", "technical"]):
            score += 0.2
        
        # å…¬å¸å®˜ç½‘
        if any(company in url for company in ["google", "microsoft", "amazon", "tesla", "nvidia"]):
            score += 0.2
        
        # åŸºç¡€è¯„åˆ†
        score += 0.1
        
        return min(score, 1.0)
    
    async def enhance_content_with_firecrawl(
        self, 
        priority_urls: List[Dict[str, Any]]
    ) -> List[Dict[str, Any]]:
        """ä½¿ç”¨Firecrawlå¢å¼ºå†…å®¹"""
        
        if not self.firecrawl_app:
            return []
        
        enhanced_results = []
        
        for url_info in priority_urls:
            url = url_info.get("url")
            if not url:
                continue
            
            try:
                print(f"ğŸ”¥ Firecrawlå¢å¼º: {url_info.get('title', 'Unknown')}")
                
                result = self.firecrawl_app.scrape_url(url)
                
                if result and result.success:
                    markdown_content = result.markdown or ''
                    
                    enhanced_results.append({
                        "url": url,
                        "title": url_info.get("title", ""),
                        "original_priority": url_info.get("priority_score", 0),
                        "enhanced_content": markdown_content,
                        "content_length": len(markdown_content),
                        "enhancement_quality": self._assess_enhancement_quality(markdown_content),
                        "source_type": "firecrawl_enhanced"
                    })
                    
                    print(f"  âœ… å¢å¼ºæˆåŠŸ: {len(markdown_content)} å­—ç¬¦")
                else:
                    print(f"  âŒ å¢å¼ºå¤±è´¥: {result.error if hasattr(result, 'error') else 'æœªçŸ¥é”™è¯¯'}")
                    
            except Exception as e:
                print(f"  âŒ å¢å¼ºå¼‚å¸¸: {str(e)}")
                continue
        
        return enhanced_results
    
    def _assess_enhancement_quality(self, content: str) -> str:
        """è¯„ä¼°å¢å¼ºå†…å®¹çš„è´¨é‡"""
        if not content:
            return "poor"
        
        length = len(content)
        has_data = any(char.isdigit() for char in content)
        has_structure = any(marker in content for marker in ['#', '##', '###'])
        
        if length > 5000 and has_data and has_structure:
            return "excellent"
        elif length > 1000 and (has_data or has_structure):
            return "good"
        elif length > 300:
            return "fair"
        else:
            return "poor"


# å»¶è¿Ÿåˆå§‹åŒ–å‡½æ•°ï¼Œé¿å…å¾ªç¯å¯¼å…¥
def get_content_enhancement_decision_maker():
    """è·å–å†…å®¹å¢å¼ºå†³ç­–å™¨å®ä¾‹ï¼ˆå»¶è¿Ÿåˆå§‹åŒ–ï¼‰"""
    if not hasattr(get_content_enhancement_decision_maker, '_instance'):
        get_content_enhancement_decision_maker._instance = ContentEnhancementDecisionMaker()
    return get_content_enhancement_decision_maker._instance

# ä¸ºäº†å‘åå…¼å®¹ï¼Œä¿ç•™åŸæœ‰çš„å…¨å±€å˜é‡å
content_enhancement_decision_maker = None  # å°†åœ¨é¦–æ¬¡ä½¿ç”¨æ—¶åˆå§‹åŒ– 